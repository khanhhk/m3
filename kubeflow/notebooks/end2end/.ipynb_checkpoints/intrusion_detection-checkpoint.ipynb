{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b508aeb4-2ef0-4bc3-9a24-3eb558d199e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.components import (\n",
    "    InputPath,\n",
    "    InputTextFile,\n",
    "    OutputPath,\n",
    "    OutputTextFile,\n",
    "    func_to_container_op,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "from typing import NamedTuple\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from constants import NAMESPACE, HOST, NAMESPACE\n",
    "from utils import get_session_cookie, get_or_create_experiment, get_or_create_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "157f76b4-8d6c-44db-b8cc-355ff91ed509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where all the runs belong to the pipeline reside in\n",
    "EXPERIMENT_NAME = \"mle-3-intrusion-detection-training\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ad1788-5a9c-4477-a5e8-f016a2eea8ea",
   "metadata": {},
   "source": [
    "## Define pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ffe13f3-8d6b-4c94-bcbb-aa3238942e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first component to download data, train-test split\n",
    "# and then dump all the data for downstream components to use\n",
    "def prepare_data(\n",
    "    X_train_path: OutputPath(\"PKL\"),\n",
    "    y_train_path: OutputPath(\"PKL\"),\n",
    "    mean_path:  OutputPath(\"MEAN\"),\n",
    "    stdev_path: OutputPath(\"STDEV\"),\n",
    ") -> int:\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import random\n",
    "    import logging\n",
    "    import os\n",
    "\n",
    "    from alibi_detect import datasets\n",
    "    from alibi_detect.utils.data import create_outlier_batch\n",
    "\n",
    "    # Load the dataset from alibi_detect\n",
    "    intrusions = datasets.fetch_kdd()\n",
    "\n",
    "    # Set seed to ensure reproducibility\n",
    "    def seed_everything(seed: int):\n",
    "        random.seed(seed)\n",
    "        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    seed_everything(42)\n",
    "\n",
    "    # Create X_train and y_train\n",
    "    logging.info(\"Creating data...\")\n",
    "    n_samples = 4000\n",
    "    normal_batch = create_outlier_batch(\n",
    "        intrusions.data, intrusions.target, n_samples=n_samples, perc_outlier=0\n",
    "    )\n",
    "    X_train, y_train = normal_batch.data.astype(float), normal_batch.target\n",
    "\n",
    "    # Preprocess X_train and y_train\n",
    "    mean, stdev = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "    X_train = (X_train - mean) / stdev\n",
    "\n",
    "    # Dump data to pkl for downstream components to use\n",
    "    logging.info(\"Dumping data...\")\n",
    "    joblib.dump(X_train, X_train_path)\n",
    "    joblib.dump(y_train, y_train_path)\n",
    "    joblib.dump(mean, mean_path)\n",
    "    joblib.dump(stdev, stdev_path)\n",
    "\n",
    "    return X_train.shape[1]\n",
    "\n",
    "# Instead of using create_component_from_func,\n",
    "# you can use this instead\n",
    "prepare_data_op = func_to_container_op(\n",
    "    func=prepare_data,\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install=[\n",
    "        \"alibi-detect[tensorflow]==0.11.1\",\n",
    "        \"alibi==0.9.1\",\n",
    "        \"urllib3==1.26.15\",\n",
    "        \"scipy==1.10.1\",\n",
    "        \"cloudpickle==2.2.1\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6339e7c-b657-4049-a57c-8369b70a6cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 14:23:40.735100: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-22 14:23:40.862383: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-22 14:23:41.377826: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2024-05-22 14:23:41.377895: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2024-05-22 14:23:41.377901: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/quandv/anaconda3/envs/kf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import intrusion_detection_model\n",
    "# import models.model\n",
    "# import models.trainer\n",
    "# from models.trainer import Trainer\n",
    "\n",
    "# The 2nd component receives outputs from the 1st component\n",
    "# and train\n",
    "def train(\n",
    "    X_train_path: InputPath(\"PKL\"),\n",
    "    y_train_path: InputPath(\"PKL\"),\n",
    "    vae_output_path: OutputPath(\"VAE\"),\n",
    "):\n",
    "    import joblib\n",
    "    from alibi_detect.models.tensorflow import elbo\n",
    "    \n",
    "    # Load data from the previous step\n",
    "    X_train = joblib.load(X_train_path)\n",
    "    y_train = joblib.load(y_train_path)\n",
    "\n",
    "    # Train a VAE model\n",
    "    n_features = X_train.shape[1]\n",
    "    latent_dim = 2\n",
    "    vae_trainer = intrusion_detection_model.Trainer(\"vae\", n_features, latent_dim)\n",
    "    vae_trainer.train(\n",
    "        X_train,\n",
    "        perc_outlier=5,\n",
    "        loss_fn=elbo,\n",
    "        cov_elbo=dict(sim=0.01),\n",
    "        epochs=1,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    # Save the model for prediction\n",
    "    vae_trainer.save_model(vae_output_path)\n",
    "\n",
    "train_op = func_to_container_op(\n",
    "    func=train,\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install=[\n",
    "        \"alibi-detect[tensorflow]==0.11.1\",\n",
    "        \"alibi==0.9.1\",\n",
    "        \"scikit-learn==1.0.2\",\n",
    "        \"joblib==1.3.2\",\n",
    "        \"cloudpickle==2.2.1\",\n",
    "        \"pandas==1.3.5\",\n",
    "        \"kfp==1.8.22\",\n",
    "        \"urllib3==1.26.15\",\n",
    "        \"scipy==1.10.1\",\n",
    "        \"requests-toolbelt==0.10.1\",  # To fix ImportError: cannot import name 'appengine' from 'urllib3.contrib'\n",
    "    ],\n",
    "    modules_to_capture=[\n",
    "        \"intrusion_detection_model\",\n",
    "    ],  # https://kubeflow-pipelines.readthedocs.io/en/1.8.13/source/kfp.components.html#kfp.components.func_to_container_op\n",
    "    use_code_pickling=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20498b63-06e1-40f5-adcf-5cddfd8f9831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 3rd component receives outputs from the 2nd component\n",
    "# in combination with prediction data from the 1st component\n",
    "# to evaluate the model\n",
    "def evaluate(\n",
    "    n_features: int,\n",
    "    mean_path: InputPath(\"MEAN\"),\n",
    "    stdev_path: InputPath(\"STDEV\"),\n",
    "    vae_output_path: InputPath(\"VAE\"),\n",
    ") -> NamedTuple(\"Outputs\", [(\"mlpipeline_metrics\", \"Metrics\"),]):\n",
    "    from alibi_detect import datasets\n",
    "    from alibi_detect.utils.data import create_outlier_batch\n",
    "\n",
    "    import joblib\n",
    "    import json\n",
    "\n",
    "    # Load the dataset from alibi_detect\n",
    "    intrusions = datasets.fetch_kdd()\n",
    "    \n",
    "    # Generate an evaluation batch, and create evaluation data\n",
    "    perc_outlier = 5\n",
    "    n_samples = 10000\n",
    "\n",
    "    # Create a batch of outlier for testing purposes\n",
    "    outlier_batch = create_outlier_batch(\n",
    "        intrusions.data,\n",
    "        intrusions.target,\n",
    "        n_samples=n_samples,\n",
    "        perc_outlier=perc_outlier,\n",
    "    )\n",
    "\n",
    "    # Get features and target\n",
    "    X_test, y_test = outlier_batch.data.astype(float), outlier_batch.target\n",
    "\n",
    "    # Normalize based on mean and stdev from the training set\n",
    "    mean = joblib.load(mean_path)\n",
    "    stdev = joblib.load(stdev_path)\n",
    "    X_test = (X_test - mean) / stdev\n",
    "\n",
    "    # Again, specify hyper-params to construct our network\n",
    "    latent_dim = 2\n",
    "\n",
    "    vae_trainer = intrusion_detection_model.Trainer(\"vae\", n_features, latent_dim)\n",
    "    vae_trainer.load_model(vae_output_path)\n",
    "    vae_preds = vae_trainer.predict(\n",
    "        X_test,\n",
    "        outlier_type=\"instance\",  # use 'feature' or 'instance' level\n",
    "        return_instance_score=True,  # Score used to determine outliers\n",
    "    )\n",
    "\n",
    "    # evaluate on the test data\n",
    "    metrics = {\n",
    "        \"metrics\": [\n",
    "            {\n",
    "                \"name\": \"f1_score\",  # The name of the metric. Visualized as the column name in the runs table.\n",
    "                \"numberValue\": vae_trainer.evaluate(\n",
    "                    y_test, vae_preds[\"data\"][\"is_outlier\"]\n",
    "                ),  # The value of the metric. Must be a numeric value.\n",
    "                \"format\": \"RAW\",  # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    return [json.dumps(metrics)]\n",
    "\n",
    "\n",
    "evaluate_op = func_to_container_op(\n",
    "    func=evaluate,\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install=[\n",
    "        \"alibi-detect[tensorflow]==0.11.1\",\n",
    "        \"alibi==0.9.1\",\n",
    "        \"cloudpickle\",\n",
    "        \"scikit-learn==1.0.2\",\n",
    "        \"joblib==1.3.2\",\n",
    "        \"pandas==1.3.5\",\n",
    "        \"kfp==1.8.22\",\n",
    "        \"urllib3==1.26.15\",\n",
    "        \"requests-toolbelt==0.10.1\",  # To fix ImportError: cannot import name 'appengine' from 'urllib3.contrib'\n",
    "    ],\n",
    "    modules_to_capture=[\n",
    "        \"intrusion_detection_model\",\n",
    "    ],  # https://kubeflow-pipelines.readthedocs.io/en/1.8.13/source/kfp.components.html#kfp.components.func_to_container_op\n",
    "    use_code_pickling=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d41eb7-5d26-49eb-b94b-55edaeadc6d1",
   "metadata": {},
   "source": [
    "## Define some pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01e4ae99-a90a-4478-89ab-4c48c8d557ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"Intrusion detection training\", description=\"Intrusion detection.\")\n",
    "def intrusion_detection_pipeline():\n",
    "    prepare_data_task = prepare_data_op()\\\n",
    "        .set_caching_options(enable_caching=True)\n",
    "        # .set_memory_request('1G')\\\n",
    "        # .set_memory_limit('2G')\\\n",
    "        # .set_display_name(name=\"Prepare Data\")\n",
    "    train_task = train_op(\n",
    "        x_train=prepare_data_task.outputs[\"X_train\"],\n",
    "        y_train=prepare_data_task.outputs[\"y_train\"],\n",
    "    )\n",
    "    evaluate_task = evaluate_op(\n",
    "        prepare_data_task.outputs[\"Output\"],\n",
    "        prepare_data_task.outputs[\"mean\"],\n",
    "        prepare_data_task.outputs[\"stdev\"],\n",
    "        vae_output=train_task.outputs[\"vae_output\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434d5cc0-7725-48b5-8cae-9f592f2f363c",
   "metadata": {},
   "source": [
    "## Run the pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "643ed18b-903e-46ee-9349-7f17b3efc250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the token to authenticate to the `ml-pipeline` service\n",
    "session_cookie = get_session_cookie()\n",
    "\n",
    "# Initialize the client\n",
    "client = kfp.Client(\n",
    "    host=f\"{HOST}/pipeline\",\n",
    "    cookies=f\"authservice_session={session_cookie}\",\n",
    "    namespace=NAMESPACE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "891bb68b-788a-4653-9b05-281db135393f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://10.10.10.10:8080/pipeline/#/experiments/details/d3c9b7c1-6e08-4486-83d4-67f09360bc01\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://10.10.10.10:8080/pipeline/#/runs/details/4a296989-60e6-4f28-a111-d693870535d3\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=4a296989-60e6-4f28-a111-d693870535d3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_run_from_pipeline_func(\n",
    "    intrusion_detection_pipeline,\n",
    "    arguments={},\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    namespace=NAMESPACE,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
