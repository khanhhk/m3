# Model configuration file (optional)
# https://github.com/triton-inference-server/tutorials/blob/main/Conceptual_Guide/Part_1-model_deployment/README.md#model-configuration
name: "yolov8n" # Give whatever name you want
backend: "onnxruntime" # Select the backend to run the model https://github.com/triton-inference-server/backend#where-can-i-find-all-the-backends-that-are-available-for-triton
max_batch_size : 8 # Max batch size the model can support
# In most cases, Triton can help to extract `input` and `output`
# but we should declare it explicitly
input [
  {
    name: "images"
    data_type: TYPE_FP32
    dims: [ 3, 640, 640 ] # If no batch, pls use [ 1, 640, 640 ]
  }
]
output [
  {
    name: "output0"
    data_type: TYPE_FP32
    dims: [ -1, -1 ] # If no batch, pls use [ 84, 8400 ]
  }
]

# dynamic_batching {
#   # You can also specify your preferred batch size (not recommended)
#   # for example in this case, it will try to collect 
#   # enough 8 requests first, if not, 4 requests. Otherwise,
#   # it will be the max batch size near 8.
#   # https://docs.nvidia.com/deeplearning/triton-inference-server/archives/triton_inference_server_220/user-guide/docs/model_configuration.html?highlight=instance_group#preferred-batch-sizes
#   # preferred_batch_size: [ 4, 8 ]
# }

# Allow "count" copies of model executors
# Refer here for more information: https://docs.nvidia.com/deeplearning/triton-inference-server/archives/triton_inference_server_220/user-guide/docs/model_configuration.html?highlight=instance_group#instance-groups
instance_group [
  { 
    count: 1 # Number of instances on each device
    kind: KIND_GPU
  } 
]

# Which models will be available for inferencing
# https://docs.nvidia.com/deeplearning/triton-inference-server/archives/triton_inference_server_220/user-guide/docs/model_configuration.html?highlight=instance_group#version-policy
# You allow all models here, other options are `latest` and `specific`
# version_policy: { all { }}
version_policy: {
    specific {
      versions: 1
      # versions: 2
    }
}

# You can further optimize with TensorRT
# https://github.com/triton-inference-server/onnxruntime_backend#onnx-runtime-with-tensorrt-optimization
# and https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html
# optimization { execution_accelerators {
#   gpu_execution_accelerator : [ {
#     name : "tensorrt"
#     # parameters { key: "precision_mode" value: "FP16" }
#     }
#   ]
# }}

# Enable cache in CPU memory for successful inference requests
# when using this feature, pay attention to how to configure the cache,
# which can be local or redis as mentioned here 
# https://github.com/triton-inference-server/server/blob/main/docs/user_guide/response_cache.md
response_cache {
  enable: false
}